{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOFO Proposal RAG System to Support the Development of Digital Health Test Beds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 0 Environment Setup\n",
    "\n",
    "In this section I install and configure all Python libraries required for my RAG pipeline:\n",
    "- sentence-transformers for generating dense embeddings of PDF text.\n",
    "- faiss-cpu for efficient similarity search over embeddings (my vector store).\n",
    "- openai for calling the LLM that will generate proposal text.\n",
    "- python-dotenv for securely loading my API key from a `.env` file.\n",
    "- PyPDF2 for extracting text from the NOFO, Application Guide, and research papers.\n",
    "\n",
    "This ensures that anyone grading or re-running my notebook can recreate the exact software environment used to build and evaluate the system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --quiet sentence-transformers faiss-cpu openai python-dotenv PyPDF2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1 Project Configuration & Paths\n",
    "\n",
    "\n",
    "- Import core Python libraries (paths, typing, NumPy).\n",
    "- Import FAISS and SentenceTransformer for embedding + retrieval.\n",
    "- Import PdfReader to read text from all PDFs.\n",
    "- Load my OPENAI_API_KEY from the `.env` file and initialize the OpenAI client.\n",
    "- Define PROJECT_DIR, PAPERS_DIR, and file paths to:\n",
    "  - The NOFO (`NOFO.pdf`) corresponding to PAR-25-136.\n",
    "  - The “How to Apply – Application Guide” PDF.\n",
    "- Print checks that confirm these paths exist.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Any, Optional\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from PyPDF2 import PdfReader\n",
    "from dotenv import load_dotenv\n",
    "import openai\n",
    "\n",
    "# Load environment variables from .env\n",
    "load_dotenv()\n",
    "\n",
    "file_name = 'config.json'\n",
    "with open(file_name, 'r') as file: \n",
    "    config = json.load(file)\n",
    "    os.environ['OPENAI_API_KEY'] = config.get(\"API_KEY\")\n",
    "    os.environ[\"OPENAI_BASE_URL\"]= config.get(\"OPENAI_API_BASE\")\n",
    "\n",
    "# Initialize OpenAI client AFTER loading env vars\n",
    "openai.api_key = api_key\n",
    "\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def call_llm(prompt: str, model: str = \"gpt-4o-mini\") -> str:\n",
    "    \"\"\"\n",
    "    Simple wrapper to call an OpenAI chat model.\n",
    "\n",
    "    - Uses a strong system message that enforces NIH / NOFO rules.\n",
    "    - Returns the assistant's text only.\n",
    "    \"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": (\n",
    "                    \"You are an expert NIH grant writer and implementation scientist. \"\n",
    "                    \"You strictly follow NIH 'How to Apply – Application Guide Research (R)' \"\n",
    "                    \"instructions and the specific NOFO requirements for \"\n",
    "                    \"'PAR-25-136: Laboratories to Optimize Digital Health (R01 Clinical Trial Required)'. \"\n",
    "                    \"You must follow ALL applicable instructions you see in the policy context. \"\n",
    "                    \"Do not fabricate citations. Only use information that is provided in the NOFO, \"\n",
    "                    \"Application Guide, and retrieved evidence corpus.\"\n",
    "                ),\n",
    "            },\n",
    "            {\"role\": \"user\", \"content\": prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content.strip()\n",
    "\n",
    "\n",
    "PROJECT_DIR = Path.home() / \"Desktop\" / \"digital_health_RAG\"\n",
    "PAPERS_DIR = Path.home() / \"Desktop\" / \"Papers\"\n",
    "\n",
    "NOFO_PATH = PROJECT_DIR / \"NOFO.pdf\"\n",
    "GUIDE_PATH = PROJECT_DIR / \"How to Apply – Application Guide _ Grants & Funding.pdf\"\n",
    "\n",
    "print(\"Project dir:\", PROJECT_DIR)\n",
    "print(\"Papers dir:\", PAPERS_DIR)\n",
    "print(\"NOFO exists?\", NOFO_PATH.exists())\n",
    "print(\"Guide exists?\", GUIDE_PATH.exists())\n",
    "print(\"OPENAI_API_KEY set?\", os.getenv(\"OPENAI_API_KEY\") is not None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2 PDF Ingestion & Chunking\n",
    "\n",
    "Here I implement reusable utilities to turn raw PDFs into text chunks that can be fed into the RAG pipeline:\n",
    "\n",
    "- load_pdf_text(path): reads all pages from a PDF and concatenates the extracted text.\n",
    "- load_papers_from_folder(folder): iterates over all PDFs in my local `Papers` directory and loads them as documents with metadata.\n",
    "- chunk_text(...): splits long documents into overlapping text chunks (e.g., 1200 characters with 200-character overlap), attaching metadata such as source filename and chunk index.\n",
    "\n",
    "Chunking is important for the RAG system because it:\n",
    "- Keeps each piece of text small enough for the embedding model and LLM context window.\n",
    "- Preserves local context via overlap, which improves retrieval quality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "def load_pdf_text(path: Path) -> str:\n",
    "    \"\"\"Read all text from a PDF file using PyPDF2.\"\"\"\n",
    "    reader = PdfReader(str(path))\n",
    "    pages = []\n",
    "    for page in reader.pages:\n",
    "        try:\n",
    "            pages.append(page.extract_text() or \"\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: could not read page in {path.name}: {e}\")\n",
    "    return \"\\n\\n\".join(pages)\n",
    "\n",
    "\n",
    "def load_papers_from_folder(folder: Path) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Load all PDFs in a folder as a list of documents.\"\"\"\n",
    "    docs = []\n",
    "    for pdf_path in folder.glob(\"*.pdf\"):\n",
    "        text = load_pdf_text(pdf_path)\n",
    "        if not text.strip():\n",
    "            print(f\"Warning: {pdf_path.name} seems empty.\")\n",
    "            continue\n",
    "        docs.append(\n",
    "            {\n",
    "                \"id\": pdf_path.name,\n",
    "                \"text\": text,\n",
    "                \"metadata\": {\"source\": \"paper\", \"filename\": pdf_path.name},\n",
    "            }\n",
    "        )\n",
    "    print(f\"Loaded {len(docs)} PDFs from {folder}\")\n",
    "    return docs\n",
    "\n",
    "\n",
    "def chunk_text(\n",
    "    text: str,\n",
    "    chunk_size: int = 1200,\n",
    "    overlap: int = 200,\n",
    "    source_id: str = \"\",\n",
    "    extra_metadata: Optional[Dict[str, Any]] = None,\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Simple character-based chunking with overlap.\n",
    "    Returns list of {\"id\", \"text\", \"metadata\"}.\n",
    "    \"\"\"\n",
    "    if extra_metadata is None:\n",
    "        extra_metadata = {}\n",
    "\n",
    "    chunks = []\n",
    "    text_len = len(text)\n",
    "    start = 0\n",
    "\n",
    "    while start < text_len:\n",
    "        end = start + chunk_size\n",
    "        chunk = text[start:end]\n",
    "\n",
    "        chunk_id = f\"{source_id}_chunk_{len(chunks)}\"\n",
    "        metadata = {\"source_id\": source_id, \"chunk_index\": len(chunks)}\n",
    "        metadata.update(extra_metadata)\n",
    "\n",
    "        chunks.append({\"id\": chunk_id, \"text\": chunk, \"metadata\": metadata})\n",
    "\n",
    "        # Move forward by chunk_size - overlap so it ALWAYS progresses\n",
    "        start += (chunk_size - overlap)\n",
    "\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3 Vector Store for Semantic Retrieval\n",
    "\n",
    "In this section I define a lightweight vector store abstraction around FAISS:\n",
    "\n",
    "- SimpleVectorStore wraps:\n",
    "  - A SentenceTransformer embedding model.\n",
    "  - A FAISS IndexFlatL2 for nearest-neighbor search.\n",
    "  - Lists of texts and their associated metadata.\n",
    "\n",
    "Key methods:\n",
    "- add_documents(docs): embeds all text chunks and adds them to the FAISS index.\n",
    "- search(query, k): retrieves the top-k most similar chunks for a user query.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class SimpleVectorStore:\n",
    "    def __init__(self, embed_model_name: str = \"all-MiniLM-L6-v2\"):\n",
    "        self.model = SentenceTransformer(embed_model_name)\n",
    "        self.index = None  # FAISS index\n",
    "        self.embeddings = None  # numpy array\n",
    "        self.texts: List[str] = []\n",
    "        self.metadatas: List[Dict[str, Any]] = []\n",
    "\n",
    "    def add_documents(self, docs: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        docs: list of {\"id\", \"text\", \"metadata\"}\n",
    "        \"\"\"\n",
    "        new_texts = [d[\"text\"] for d in docs]\n",
    "        new_metas = [d[\"metadata\"] for d in docs]\n",
    "\n",
    "        print(f\"Embedding {len(new_texts)} chunks...\")\n",
    "        new_embs = self.model.encode(new_texts, show_progress_bar=True, convert_to_numpy=True)\n",
    "\n",
    "        if self.index is None:\n",
    "            d = new_embs.shape[1]\n",
    "            self.index = faiss.IndexFlatL2(d)\n",
    "            self.embeddings = new_embs\n",
    "        else:\n",
    "            self.embeddings = np.vstack([self.embeddings, new_embs])\n",
    "\n",
    "        self.index.add(new_embs)\n",
    "        self.texts.extend(new_texts)\n",
    "        self.metadatas.extend(new_metas)\n",
    "\n",
    "    def search(self, query: str, k: int = 5) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Return top-k chunks most similar to the query.\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"Vector store is empty. Add documents first.\")\n",
    "\n",
    "        q_emb = self.model.encode([query], convert_to_numpy=True)\n",
    "        distances, indices = self.index.search(q_emb, k)\n",
    "\n",
    "        results = []\n",
    "        for dist, idx in zip(distances[0], indices[0]):\n",
    "            results.append(\n",
    "                {\n",
    "                    \"text\": self.texts[int(idx)],\n",
    "                    \"metadata\": self.metadatas[int(idx)],\n",
    "                    \"score\": float(dist),\n",
    "                }\n",
    "            )\n",
    "        return results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4 Reading NOFO & Application Guide (Policy Corpus)\n",
    "\n",
    "\n",
    "Here I:\n",
    "\n",
    "1. Load the full text of:\n",
    "   - The PAR-25-136 NOFO document.\n",
    "   - The NIH “How to Apply – Application Guide” (Research (R) instructions).\n",
    "2. Chunk each document using chunk_text, tagging chunks as type=\"policy\".\n",
    "3. Build policy_store, a vector store containing all NOFO + Application Guide chunks.\n",
    "\n",
    "This policy corpus is used later to:\n",
    "- Ensure my generated proposals follow the NOFO’s scientific scope (digital health test beds, mental health, health disparities, etc.).\n",
    "- Enforce formatting and structural rules from the Application Guide (e.g., Specific Aims, Research Strategy with Significance/Innovation/Approach).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load NOFO and Application Guide text\n",
    "nofo_text = load_pdf_text(NOFO_PATH)\n",
    "guide_text = load_pdf_text(GUIDE_PATH)\n",
    "\n",
    "policy_docs = [\n",
    "    {\"id\": \"NOFO\", \"text\": nofo_text, \"metadata\": {\"source\": \"NOFO\", \"type\": \"policy\"}},\n",
    "    {\"id\": \"HOW_TO_APPLY_GUIDE\", \"text\": guide_text, \"metadata\": {\"source\": \"guide\", \"type\": \"policy\"}},\n",
    "]\n",
    "\n",
    "# Chunk both policy documents\n",
    "policy_chunks = []\n",
    "for doc in policy_docs:\n",
    "    chunks = chunk_text(\n",
    "        doc[\"text\"],\n",
    "        chunk_size=1200,\n",
    "        overlap=200,\n",
    "        source_id=doc[\"id\"],\n",
    "        extra_metadata={\"source\": doc[\"metadata\"][\"source\"], \"type\": \"policy\"},\n",
    "    )\n",
    "    policy_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total policy chunks: {len(policy_chunks)}\")\n",
    "\n",
    "policy_store = SimpleVectorStore()\n",
    "policy_store.add_documents(policy_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5 Analyzing Past Research Papers (Evidence Corpus)\n",
    "\n",
    "In this section I:\n",
    "\n",
    "1. Call load_papers_from_folder(PAPERS_DIR) to load all PDFs from my local repository of previous work and research ideas.\n",
    "2. Use chunk_text to break each paper into overlapping text chunks, attaching metadata such as filename and type=\"research_paper\".\n",
    "3. Build research_store, a vector store that contains all research-evidence chunks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "research_docs = load_papers_from_folder(PAPERS_DIR)\n",
    "\n",
    "# Chunk each paper\n",
    "research_chunks = []\n",
    "for doc in research_docs:\n",
    "    chunks = chunk_text(\n",
    "        doc[\"text\"],\n",
    "        chunk_size=1200,\n",
    "        overlap=200,\n",
    "        source_id=doc[\"id\"],\n",
    "        extra_metadata={\"filename\": doc[\"metadata\"][\"filename\"], \"type\": \"research_paper\"},\n",
    "    )\n",
    "    research_chunks.extend(chunks)\n",
    "\n",
    "print(f\"Total research chunks: {len(research_chunks)}\")\n",
    "\n",
    "# Build the vector store\n",
    "research_store = SimpleVectorStore()\n",
    "research_store.add_documents(research_chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6 Retrieval Functions (Linking Policy + Evidence to the LLM)\n",
    "\n",
    "This section defines helper functions for the RAG retrieval step:\n",
    "\n",
    "- retrieve_context(goal_description, k_research, k_policy):\n",
    "  - Uses research_store to retrieve the most relevant research paper chunks.\n",
    "  - Uses policy_store to retrieve the most relevant NOFO/Application Guide chunks.\n",
    "- format_context_for_prompt(results):\n",
    "  - Formats the retrieved chunks into a structured text block with labels\n",
    "    (e.g., [POLICY 1], [RESEARCH 2]) that will be inserted into the LLM prompt.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def retrieve_context(\n",
    "    goal_description: str,\n",
    "    k_research: int = 8,\n",
    "    k_policy: int = 8,\n",
    ") -> Dict[str, List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Given a high-level goal/prompt, pull top chunks from research + policy stores.\n",
    "    \"\"\"\n",
    "    research_results = research_store.search(goal_description, k=k_research)\n",
    "    policy_results = policy_store.search(goal_description, k=k_policy)\n",
    "\n",
    "    return {\n",
    "        \"research\": research_results,\n",
    "        \"policy\": policy_results,\n",
    "    }\n",
    "\n",
    "\n",
    "def format_context_for_prompt(results: Dict[str, List[Dict[str, Any]]]) -> str:\n",
    "    \"\"\"\n",
    "    Turn retrieved chunks into a compact context block for the LLM.\n",
    "    \"\"\"\n",
    "    lines = []\n",
    "\n",
    "    lines.append(\"POLICY & INSTRUCTIONS (NOFO + How to Apply Guide) \")\n",
    "    for i, r in enumerate(results[\"policy\"], start=1):\n",
    "        src = r[\"metadata\"].get(\"source\", \"\")\n",
    "        lines.append(f\"[POLICY {i} | source={src}]\")\n",
    "        lines.append(r[\"text\"].strip())\n",
    "        lines.append(\"\")\n",
    "\n",
    "    lines.append(\"SCIENTIFIC EVIDENCE & BACKGROUND (Research Papers) \")\n",
    "    for i, r in enumerate(results[\"research\"], start=1):\n",
    "        fname = r[\"metadata\"].get(\"filename\", \"\")\n",
    "        lines.append(f\"[RESEARCH {i} | file={fname}]\")\n",
    "        lines.append(r[\"text\"].strip())\n",
    "        lines.append(\"\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7 Goal Definition & RAG Retrieval\n",
    "\n",
    "In this section I:\n",
    "\n",
    "- Define a **high-level goal description** that reflects the scientific and technical focus of the NOFO on digital mental health test beds.\n",
    "- Use my RAG retrieval pipeline to:\n",
    "  - Search the **policy corpus** (NOFO + Application Guide) for key requirements and constraints.\n",
    "  - Search the **evidence corpus** (past research papers) for relevant prior work and methods.\n",
    "- Format the retrieved content into a compact context block that will be fed into the LLM in later sections.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "goal_description = \"\"\"\n",
    "Design and evaluate a digital health test bed to optimize real-world\n",
    "deployment of AI-enabled tools for behavioral and mental health.\n",
    "The system should support diverse patient populations, integrate\n",
    "with existing clinical workflows, and generate evaluable outcomes\n",
    "aligned with PAR-25-136: Laboratories to Optimize Digital Health\n",
    "(R01 Clinical Trial Required).\n",
    "\"\"\".strip()\n",
    "\n",
    "# Number of chunks to pull from each corpus\n",
    "K_POLICY = 8\n",
    "K_RESEARCH = 8\n",
    "\n",
    "results = retrieve_context(\n",
    "    goal_description=goal_description,\n",
    "    k_research=K_RESEARCH,\n",
    "    k_policy=K_POLICY,\n",
    ")\n",
    "\n",
    "context_block = format_context_for_prompt(results)\n",
    "\n",
    "print(\"Goal Description\")\n",
    "print(goal_description)\n",
    "print(\"\\nRetrieved Context (Policy + Research) \\n\")\n",
    "print(context_block[:4000], \"...\\n\")  # truncate for display\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8 Generating Research Ideas (LLM Processing – Step 1)\n",
    "\n",
    "In this section I:\n",
    "\n",
    "- Use the **retrieved policy + evidence context** to generate structured research ideas.\n",
    "- Ask the LLM to propose several candidate project concepts that:\n",
    "  - Satisfy key NOFO requirements.\n",
    "  - Leverage insights from my prior research papers.\n",
    "  - Are feasible for an R01 digital health test bed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_research_ideas(goal: str, context: str, n_ideas: int = 5) -> str:\n",
    "    \"\"\"\n",
    "    Use the RAG context to generate several candidate research ideas\n",
    "    that respond to the NOFO and leverage prior work.\n",
    "    Returns a formatted string of ideas.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an NIH implementation scientist and grant writer.\n",
    "\n",
    "You are helping to design a **digital health test bed** grant proposal\n",
    "for PAR-25-136: Laboratories to Optimize Digital Health (R01 Clinical Trial Required).\n",
    "\n",
    "First, carefully read the following **GOAL DESCRIPTION** and **RETRIEVED CONTEXT**.\n",
    "\n",
    "GOAL DESCRIPTION:\n",
    "\\\"\\\"\\\"{goal}\\\"\\\"\\\"\n",
    "\n",
    "RETRIEVED CONTEXT (Policy + Research):\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "TASK:\n",
    "1. Propose exactly {n_ideas} distinct, numbered **research ideas** that:\n",
    "   - Align with the NOFO’s intent, eligibility, and review criteria.\n",
    "   - Use or extend methods / findings that appear in the research corpus.\n",
    "   - Are realistic for an R01 test bed (multi-site or robust single-site, pragmatic design).\n",
    "2. For each idea, provide:\n",
    "   - A short title.\n",
    "   - A 3–5 sentence description.\n",
    "   - 2–3 bullet points on how it meets NOFO priorities (e.g., digital mental health, health disparities, implementation focus).\n",
    "\n",
    "Return your answer in a clearly formatted, numbered list.\n",
    "\"\"\"\n",
    "    ideas_text = call_llm(prompt)\n",
    "    return ideas_text\n",
    "\n",
    "\n",
    "research_ideas = generate_research_ideas(goal_description, context_block, n_ideas=5)\n",
    "\n",
    "print(\"Generated Research Ideas \\n\")\n",
    "print(research_ideas)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9 Drafting a Technical Approach (LLM Processing – Step 2)\n",
    "\n",
    "In this section I:\n",
    "\n",
    "- Select and refine one of the research ideas as the **primary project concept**.\n",
    "- Ask the LLM to draft a structured **Technical Approach** section that:\n",
    "  - Is tailored to PAR-25-136 and NIH R01 conventions.\n",
    "  - Uses the RAG context from the NOFO, Application Guide, and prior research.\n",
    "  - Is organized into standard NIH-style headings (e.g., Overview, Significance, Innovation, Approach).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chosen_idea_number = 1\n",
    "\n",
    "prompt_for_approach = f\"\"\"\n",
    "You previously generated the following **candidate research ideas**:\n",
    "\n",
    "\\\"\\\"\\\"{research_ideas}\\\"\\\"\\\"\n",
    "\n",
    "Focus on **Idea {chosen_idea_number}** as the primary project concept.\n",
    "\n",
    "You also have the following:\n",
    "- GOAL DESCRIPTION:\n",
    "\\\"\\\"\\\"{goal_description}\\\"\\\"\\\"\n",
    "\n",
    "- RETRIEVED CONTEXT (Policy + Research):\n",
    "\\\"\\\"\\\"{context_block}\\\"\\\"\\\"\n",
    "\n",
    "TASK:\n",
    "Draft a detailed, but concise **Technical Approach** for a 5-page NIH-style\n",
    "proposal responding to PAR-25-136: Laboratories to Optimize Digital Health (R01 Clinical Trial Required).\n",
    "\n",
    "Requirements:\n",
    "- Assume this section will later be edited and formatted into a ≤5-page PDF.\n",
    "- Use clear headings and subheadings such as:\n",
    "  - Overview / Project Summary\n",
    "  - Significance and Innovation\n",
    "  - Test Bed Design and Sites\n",
    "  - Participants and Recruitment\n",
    "  - Digital Health Intervention / Platform\n",
    "  - Study Design and Methods\n",
    "  - Data Collection and Outcomes\n",
    "  - Analysis Plan\n",
    "  - Implementation and Scalability\n",
    "  - Human Subjects Protections and Data Security (high-level)\n",
    "  - Timeline and Milestones\n",
    "- Explicitly reference alignment with NOFO priorities (e.g., digital mental health, test bed focus, health disparities, implementation).\n",
    "- Integrate insights from the prior research corpus when relevant (e.g., study designs, measures, analytic strategies).\n",
    "\n",
    "Write in professional NIH grant language, but keep the draft readable enough that a human can later trim and reformat.\n",
    "\"\"\"\n",
    "\n",
    "draft_technical_approach = call_llm(prompt_for_approach)\n",
    "\n",
    "print(\"Draft Technical Approach (Version 1) \\n\")\n",
    "print(draft_technical_approach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10 LLM Evaluation and Iterative Refinement\n",
    "\n",
    "In this section I:\n",
    "\n",
    "- Ask the LLM to **critically evaluate** the draft technical approach against:\n",
    "  - NOFO requirements and review criteria.\n",
    "  - Clarity, coherence, feasibility, and alignment with the digital health test bed concept.\n",
    "- Request **specific revision suggestions**, including missing elements or misalignments.\n",
    "- Generate a **revised version** of the technical approach that incorporates this feedback."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate_proposal(technical_approach: str, goal: str, context: str) -> str:\n",
    "    \"\"\"\n",
    "    Ask the LLM to critique the draft technical approach and identify\n",
    "    strengths, weaknesses, and specific revision suggestions.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "You are an NIH study section reviewer specializing in digital mental health.\n",
    "\n",
    "Review the following draft **Technical Approach** for an R01 proposal\n",
    "responding to PAR-25-136: Laboratories to Optimize Digital Health (R01 Clinical Trial Required).\n",
    "\n",
    "GOAL DESCRIPTION:\n",
    "\\\"\\\"\\\"{goal}\\\"\\\"\\\"\n",
    "\n",
    "RETRIEVED CONTEXT (Policy + Research):\n",
    "\\\"\\\"\\\"{context}\\\"\\\"\\\"\n",
    "\n",
    "DRAFT TECHNICAL APPROACH:\n",
    "\\\"\\\"\\\"{technical_approach}\\\"\\\"\\\"\n",
    "\n",
    "TASK:\n",
    "Provide a structured review with the following sections:\n",
    "\n",
    "1. **Overall Assessment** (1–2 paragraphs).\n",
    "2. **Strengths** (bullet points).\n",
    "3. **Weaknesses / Gaps** (bullet points).\n",
    "4. **Specific Recommendations for Revision**:\n",
    "   - Missing elements relative to the NOFO or Application Guide.\n",
    "   - Areas that need greater methodological detail or clarity.\n",
    "   - Potential concerns about feasibility, innovation, or significance.\n",
    "5. **Priority Revisions**:\n",
    "   - A short list (3–5 items) of the most important changes to address before submission.\n",
    "\n",
    "Write in a constructive, reviewer-style tone.\n",
    "\"\"\"\n",
    "    review_text = call_llm(prompt)\n",
    "    return review_text\n",
    "\n",
    "\n",
    "proposal_review = evaluate_proposal(\n",
    "    technical_approach=draft_technical_approach,\n",
    "    goal=goal_description,\n",
    "    context=context_block,\n",
    ")\n",
    "\n",
    "print(\"LLM Review of Draft Technical Approach\\n\")\n",
    "print(proposal_review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_for_revision = f\"\"\"\n",
    "You are the same NIH grant writer who drafted the previous technical approach.\n",
    "\n",
    "You just received the following **review and recommendations**:\n",
    "\n",
    "\\\"\\\"\\\"{proposal_review}\\\"\\\"\\\"\n",
    "\n",
    "Original DRAFT TECHNICAL APPROACH:\n",
    "\\\"\\\"\\\"{draft_technical_approach}\\\"\\\"\\\"\n",
    "\n",
    "TASK:\n",
    "Produce a **revised Technical Approach (Version 2)** that:\n",
    "\n",
    "- Incorporates the most important reviewer suggestions.\n",
    "- Improves alignment with PAR-25-136 and NIH R01 expectations.\n",
    "- Clarifies methods, study design, and implementation details where requested.\n",
    "- Keeps the structure suitable for a ≤5-page Technical Approach section.\n",
    "\n",
    "Do NOT write a summary of the changes.  \n",
    "Instead, output only the **revised Technical Approach**, fully written out with clear headings and subheadings.\n",
    "\"\"\"\n",
    "\n",
    "revised_technical_approach = call_llm(prompt_for_revision)\n",
    "\n",
    "print(\"Revised Technical Approach (Version 2) \\n\")\n",
    "print(revised_technical_approach)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 11 Saving Proposal Outputs for Human Evaluation\n",
    "\n",
    "In this final technical step I:\n",
    "\n",
    "- Save the **goal description**, **selected idea**, **LLM-generated technical approaches**, and **review** to text files.\n",
    "- These files serve as the raw materials that I will manually edit and format into the final ≤5-page PDF Technical Approach required by the assignment.\n",
    "\n",
    "This corresponds to the **\"Human Evaluation\"** and **\"Final Proposal\"** steps in the workflow, where I:\n",
    "- Review the LLM output.\n",
    "- Make any necessary conceptual, methodological, or formatting changes.\n",
    "- Export the final version as a PDF for submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_DIR = PROJECT_DIR / \"outputs\"\n",
    "OUTPUT_DIR.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "files_to_write = {\n",
    "    \"goal_description.txt\": goal_description,\n",
    "    \"retrieved_context.txt\": context_block,\n",
    "    \"research_ideas.txt\": research_ideas,\n",
    "    \"draft_technical_approach_v1.txt\": draft_technical_approach,\n",
    "    \"proposal_review.txt\": proposal_review,\n",
    "    \"revised_technical_approach_v2.txt\": revised_technical_approach,\n",
    "}\n",
    "\n",
    "for fname, text in files_to_write.items():\n",
    "    out_path = OUTPUT_DIR / fname\n",
    "    with open(out_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(text)\n",
    "    print(f\"Wrote: {out_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Algorithms-Kernel)",
   "language": "python",
   "name": "en-685-621"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
