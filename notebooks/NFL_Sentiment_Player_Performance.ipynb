{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Social Media Sentiment and NFL Player Performance\n",
    "\n",
    "**Objective:** Analyze sentiment in Reddit threads from the NFL Draft to explore its predictive power in evaluating future player performance. Use NLP and sentiment models to extract insights and assess their value for sports analytics and decision‑making.\n",
    "\n",
    "**Key Files (edit paths below if yours differ):**\n",
    "- `NFL_reddit_data_2021.csv` — Reddit comments / posts\n",
    "- `combined_player_data.csv` — Player roster + stats/grades\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you're running in a fresh environment, uncomment and run this once.\n",
    "# %pip install -q pandas numpy matplotlib scikit-learn vaderSentiment rapidfuzz python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from dateutil import tz\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from rapidfuzz import process, fuzz\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "# ---- Paths (EDIT THESE if your filenames differ) ----\n",
    "NFL_REDDIT_PATH = \"NFL_reddit_data_2021.csv\"\n",
    "PLAYER_DATA_PATH = \"combined_player_data.csv\"\n",
    "\n",
    "# Where to save derived artifacts\n",
    "ARTIFACT_DIR = \"artifacts\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load Data ---\n",
    "def try_read_csv(path):\n",
    "    for enc in [None, \"utf-8\", \"utf-8-sig\", \"latin-1\"]:\n",
    "        try:\n",
    "            return pd.read_csv(path, encoding=enc)\n",
    "        except Exception as e:\n",
    "            last = e\n",
    "    raise last\n",
    "\n",
    "reddit_df = try_read_csv(NFL_REDDIT_PATH)\n",
    "players_df = try_read_csv(PLAYER_DATA_PATH)\n",
    "\n",
    "print(\"Reddit shape:\", reddit_df.shape)\n",
    "print(\"Players shape:\", players_df.shape)\n",
    "print(\"Reddit columns:\", reddit_df.columns.tolist()[:25])\n",
    "print(\"Players columns:\", players_df.columns.tolist()[:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Preprocess Reddit text & timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Preprocessing & Cleaning ---\n",
    "# Heuristically identify text and time columns\n",
    "def find_text_column(df):\n",
    "    candidates = [\"body\",\"text\",\"comment\",\"content\",\"selftext\",\"title\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns: \n",
    "            return c\n",
    "    # fallback: pick the first object/string column with average length > 20\n",
    "    for c in df.select_dtypes(\"object\").columns:\n",
    "        if df[c].dropna().astype(str).map(len).mean() > 20:\n",
    "            return c\n",
    "    raise ValueError(\"Could not find a text column. Please rename one to 'body'.\")\n",
    "\n",
    "def find_datetime_column(df):\n",
    "    candidates = [\"created_utc\",\"created_at\",\"created\",\"timestamp\",\"date\",\"time\"]\n",
    "    for c in candidates:\n",
    "        if c in df.columns:\n",
    "            dt = pd.to_datetime(df[c], errors=\"coerce\", utc=True)\n",
    "            if dt.notna().mean() > 0.2:\n",
    "                return c\n",
    "    # none found\n",
    "    return None\n",
    "\n",
    "TEXT_COL = find_text_column(reddit_df)\n",
    "TIME_COL = find_datetime_column(reddit_df)\n",
    "\n",
    "# Basic cleaning: drop duplicates, normalize text\n",
    "reddit_df = reddit_df.drop_duplicates()\n",
    "reddit_df[TEXT_COL] = reddit_df[TEXT_COL].astype(str).str.strip()\n",
    "\n",
    "# Remove obvious empties\n",
    "reddit_df = reddit_df[reddit_df[TEXT_COL].str.len() > 0].copy()\n",
    "\n",
    "# Parse time if available\n",
    "if TIME_COL:\n",
    "    reddit_df[\"created_dt\"] = pd.to_datetime(reddit_df[TIME_COL], errors=\"coerce\", utc=True).dt.tz_convert(\"UTC\")\n",
    "else:\n",
    "    reddit_df[\"created_dt\"] = pd.NaT\n",
    "\n",
    "# Simple text normalization helpful for downstream matching\n",
    "url_pat = re.compile(r\"https?://\\S+|www\\.\\S+\")\n",
    "reddit_df[\"text_clean\"] = (\n",
    "    reddit_df[TEXT_COL]\n",
    "    .str.replace(url_pat, \" \", regex=True)\n",
    "    .str.replace(r\"[\\n\\r\\t]\", \" \", regex=True)\n",
    "    .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")\n",
    "\n",
    "reddit_df.to_csv(f\"{ARTIFACT_DIR}/reddit_clean.csv\", index=False)\n",
    "reddit_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Prepare player roster and name lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Prepare Player Roster (canonical names) ---\n",
    "# Try to infer a canonical full name column 'player' from the player file\n",
    "name_cols = [c for c in players_df.columns if c.lower() in {\"player\",\"player_name\",\"name\",\"full_name\"}]\n",
    "if len(name_cols) == 0:\n",
    "    # Try first/last name combination\n",
    "    first = next((c for c in players_df.columns if c.lower() in {\"first\",\"first_name\"}), None)\n",
    "    last = next((c for c in players_df.columns if c.lower() in {\"last\",\"last_name\",\"surname\"}), None)\n",
    "    if first and last:\n",
    "        players_df[\"player\"] = (players_df[first].astype(str).str.strip() + \" \" + players_df[last].astype(str).str.strip()).str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    else:\n",
    "        raise ValueError(\"Couldn't infer player name column. Ensure one of ['player','player_name','name'] exists or both first/last name columns exist.\")\n",
    "else:\n",
    "    players_df[\"player\"] = players_df[name_cols[0]].astype(str).str.strip()\n",
    "\n",
    "# Create helper columns for matching\n",
    "players_df[\"player_lower\"] = players_df[\"player\"].str.lower()\n",
    "\n",
    "# Build last-name and full-name lexicons\n",
    "def last_name(full_name):\n",
    "    toks = [t for t in str(full_name).split() if t.strip()]\n",
    "    return toks[-1] if toks else \"\"\n",
    "\n",
    "players_df[\"last_name\"] = players_df[\"player\"].map(last_name).str.lower()\n",
    "\n",
    "# Only use last names that are unique to avoid false positives\n",
    "last_counts = players_df[\"last_name\"].value_counts()\n",
    "unique_last_names = set(last_counts[last_counts == 1].index)\n",
    "\n",
    "# Canonical dictionary: variant -> canonical full name\n",
    "name_map = {}\n",
    "\n",
    "# Include full names always\n",
    "for nm in players_df[\"player\"].unique():\n",
    "    name_map[nm.lower()] = nm\n",
    "\n",
    "# Include unique last names\n",
    "for _, r in players_df.iterrows():\n",
    "    if r[\"last_name\"] in unique_last_names and r[\"last_name\"]:\n",
    "        name_map[r[\"last_name\"]] = r[\"player\"]\n",
    "\n",
    "# Build a compiled regex for fast matching\n",
    "# Order names by length desc to prefer full names first\n",
    "alternatives = sorted(list(name_map.keys()), key=lambda s: len(s), reverse=True)\n",
    "# Escape regex metachars and join\n",
    "alts_escaped = [re.escape(a) for a in alternatives if a and isinstance(a, str)]\n",
    "if len(alts_escaped) == 0:\n",
    "    raise ValueError(\"No names found to match against. Check your player file.\")\n",
    "\n",
    "pattern = re.compile(r\"\\b(\" + \"|\".join(alts_escaped) + r\")\\b\", flags=re.IGNORECASE)\n",
    "\n",
    "len(name_map), list(name_map.items())[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5) Match Reddit comments to players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Link Reddit comments to players via name matching ---\n",
    "def find_matches(text):\n",
    "    if not isinstance(text, str) or not text:\n",
    "        return []\n",
    "    return [name_map.get(m.group(0).lower(), m.group(0)) for m in pattern.finditer(text)]\n",
    "\n",
    "reddit_df[\"matched_players\"] = reddit_df[\"text_clean\"].map(find_matches)\n",
    "\n",
    "# Explode to one row per (comment x player)\n",
    "mentions = reddit_df.explode(\"matched_players\").rename(columns={\"matched_players\":\"player\"})\n",
    "mentions = mentions[mentions[\"player\"].notna() & (mentions[\"player\"].astype(str).str.len() > 0)].copy()\n",
    "\n",
    "print(\"Mentions shape:\", mentions.shape)\n",
    "mentions.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6) Sentiment analysis with VADER + aggregation by player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sentiment Analysis (VADER) ---\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "def vader_compound(text):\n",
    "    if not isinstance(text, str) or text.strip() == \"\":\n",
    "        return np.nan\n",
    "    return analyzer.polarity_scores(text)[\"compound\"]\n",
    "\n",
    "mentions[\"compound\"] = mentions[\"text_clean\"].map(vader_compound)\n",
    "\n",
    "def label_from_compound(c):\n",
    "    if pd.isna(c):\n",
    "        return \"unknown\"\n",
    "    if c >= 0.05:\n",
    "        return \"positive\"\n",
    "    if c <= -0.05:\n",
    "        return \"negative\"\n",
    "    return \"neutral\"\n",
    "\n",
    "mentions[\"sentiment_label\"] = mentions[\"compound\"].map(label_from_compound)\n",
    "\n",
    "# Aggregate by player\n",
    "agg = (mentions\n",
    "       .assign(week=mentions[\"created_dt\"].dt.to_period(\"W\").astype(str) if \"created_dt\" in mentions.columns else \"unknown\")\n",
    "       .groupby(\"player\")\n",
    "       .agg(mentions=(\"compound\",\"count\"),\n",
    "            mean_compound=(\"compound\",\"mean\"),\n",
    "            median_compound=(\"compound\",\"median\"),\n",
    "            pos_rate=(lambda x: (x >= 0.05).mean()),\n",
    "            neg_rate=(lambda x: (x <= -0.05).mean())\n",
    "       )\n",
    "       .reset_index()\n",
    "      )\n",
    "\n",
    "agg = agg.sort_values(\"mentions\", ascending=False)\n",
    "agg.to_csv(f\"{ARTIFACT_DIR}/sentiment_by_player.csv\", index=False)\n",
    "agg.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7) Build a player performance score and join with sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Player Performance Scoring ---\n",
    "# Try to auto-detect a single performance column; otherwise build a composite score.\n",
    "\n",
    "def pick_performance_column(df):\n",
    "    candidates = [\"overall_grade\",\"overall\",\"pff_grade\",\"rating\",\"fantasy_points\",\"approximate_value\",\"av\",\"war\",\"epa\"]\n",
    "    lc = {c.lower(): c for c in df.columns}\n",
    "    for cand in candidates:\n",
    "        if cand in lc:\n",
    "            return lc[cand]\n",
    "    return None\n",
    "\n",
    "# Standardize utility\n",
    "def zscore(s):\n",
    "    return (s - s.mean()) / (s.std(ddof=0) + 1e-9)\n",
    "\n",
    "perf_col = pick_performance_column(players_df)\n",
    "\n",
    "# Prepare a canonical 'player' column already present\n",
    "perf_df = players_df.copy()\n",
    "\n",
    "if perf_col is not None and np.issubdtype(perf_df[perf_col].dtype, np.number):\n",
    "    perf_df[\"PerformanceScore\"] = zscore(perf_df[perf_col])\n",
    "    used_cols = [perf_col]\n",
    "else:\n",
    "    # Build a composite score from numeric columns that look like performance\n",
    "    num_cols = perf_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    drop_keywords = [\"id\",\"year\",\"season\",\"age\",\"height\",\"weight\",\"draft\",\"round\",\"pick\",\"team_id\",\"position_id\"]\n",
    "    perf_cols = [c for c in num_cols if not any(k in c.lower() for k in drop_keywords)]\n",
    "    negative_keywords = [\"fumble\",\"interception\",\"interceptions\",\"int\",\"penalty\",\"drop\",\"sack_allowed\",\"missed_tackle\"]\n",
    "\n",
    "    if len(perf_cols) == 0:\n",
    "        raise ValueError(\"No numeric performance-like columns found. Please add/select a performance column.\")\n",
    "\n",
    "    Z = pd.DataFrame({c: zscore(perf_df[c].astype(float)) for c in perf_cols})\n",
    "    signs = np.array([ -1.0 if any(k in c.lower() for k in negative_keywords) else 1.0 for c in perf_cols ])\n",
    "    perf_df[\"PerformanceScore\"] = (Z.values * signs).mean(axis=1)\n",
    "    used_cols = perf_cols[:15]  # show a subset in the report to keep it readable\n",
    "\n",
    "# Keep only what we need\n",
    "perf_df_small = perf_df[[\"player\",\"PerformanceScore\"]].copy()\n",
    "\n",
    "# Join sentiment with performance\n",
    "player_scores = perf_df_small.merge(agg, on=\"player\", how=\"left\")\n",
    "player_scores.to_csv(f\"{ARTIFACT_DIR}/player_scores.csv\", index=False)\n",
    "player_scores.head(10), perf_col, used_cols[:8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8) Evaluation & Visualizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Evaluation & Visualizations ---\n",
    "from scipy.stats import pearsonr, spearmanr\n",
    "\n",
    "valid = player_scores.dropna(subset=[\"PerformanceScore\",\"mean_compound\"]).copy()\n",
    "\n",
    "pearson_r = np.nan\n",
    "spearman_r = np.nan\n",
    "if len(valid) >= 3:\n",
    "    pearson_r = pearsonr(valid[\"mean_compound\"], valid[\"PerformanceScore\"])[0]\n",
    "    spearman_r = spearmanr(valid[\"mean_compound\"], valid[\"PerformanceScore\"]).correlation\n",
    "\n",
    "print(f\"Pearson r (sentiment vs performance): {pearson_r:.3f}\" if not math.isnan(pearson_r) else \"Pearson r: N/A\")\n",
    "print(f\"Spearman r (rank correlation): {spearman_r:.3f}\" if not math.isnan(spearman_r) else \"Spearman r: N/A\")\n",
    "\n",
    "# Precision@k for \"top performers\" predicted by sentiment\n",
    "def precision_at_k(df, k=10, sentiment_col=\"mean_compound\"):\n",
    "    df = df.dropna(subset=[\"PerformanceScore\", sentiment_col]).copy()\n",
    "    if len(df) == 0 or k <= 0:\n",
    "        return np.nan\n",
    "    top_sent = set(df.sort_values(sentiment_col, ascending=False).head(k)[\"player\"])\n",
    "    top_perf = set(df.sort_values(\"PerformanceScore\", ascending=False).head(k)[\"player\"])\n",
    "    return len(top_sent & top_perf) / float(k)\n",
    "\n",
    "for k in [5,10,20]:\n",
    "    p_at_k = precision_at_k(valid, k=k, sentiment_col=\"mean_compound\")\n",
    "    print(f\"precision@{k}: {p_at_k:.3f}\" if not pd.isna(p_at_k) else f\"precision@{k}: N/A\")\n",
    "\n",
    "# --- Plots ---\n",
    "# 1) Sentiment vs Performance scatter\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.scatter(valid[\"mean_compound\"], valid[\"PerformanceScore\"])\n",
    "plt.axvline(0, linestyle=\"--\", linewidth=1)\n",
    "plt.title(\"Sentiment (mean compound) vs Player Performance\")\n",
    "plt.xlabel(\"Mean VADER Compound Sentiment\")\n",
    "plt.ylabel(\"PerformanceScore (standardized)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{ARTIFACT_DIR}/sentiment_vs_performance.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 2) Top mentioned players bar chart\n",
    "topN = 20\n",
    "top_mentions = player_scores.sort_values(\"mentions\", ascending=False).head(topN)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(top_mentions[\"player\"][::-1], top_mentions[\"mentions\"][::-1])\n",
    "plt.title(f\"Top {topN} Players by Reddit Mentions\")\n",
    "plt.xlabel(\"Mentions\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{ARTIFACT_DIR}/top_mentions.png\", dpi=160)\n",
    "plt.show()\n",
    "\n",
    "# 3) Distribution of compound sentiment\n",
    "plt.figure(figsize=(8,6))\n",
    "plt.hist(mentions[\"compound\"].dropna(), bins=50, alpha=0.8, density=False)\n",
    "plt.title(\"Distribution of VADER Compound Scores for Player Mentions\")\n",
    "plt.xlabel(\"Compound\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{ARTIFACT_DIR}/compound_distribution.png\", dpi=160)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9) Weekly sentiment trends for top‑mentioned players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Sentiment Trend Over Time (Weekly) ---\n",
    "if \"created_dt\" in mentions.columns and mentions[\"created_dt\"].notna().any():\n",
    "    # focus on players with enough volume\n",
    "    vol = agg[agg[\"mentions\"] >= max(5, agg[\"mentions\"].quantile(0.75))][\"player\"].tolist()\n",
    "    m2 = mentions[mentions[\"player\"].isin(vol)].copy()\n",
    "    m2[\"week\"] = m2[\"created_dt\"].dt.to_period(\"W\").dt.start_time\n",
    "    trend = m2.groupby([\"player\",\"week\"])[\"compound\"].mean().reset_index()\n",
    "\n",
    "    # plot per top player\n",
    "    for p in vol[:6]:  # cap to 6 plots to keep manageable\n",
    "        sub = trend[trend[\"player\"] == p]\n",
    "        plt.figure(figsize=(8,4))\n",
    "        plt.plot(sub[\"week\"], sub[\"compound\"], marker=\"o\")\n",
    "        plt.axhline(0, linestyle=\"--\", linewidth=1)\n",
    "        plt.title(f\"Weekly Sentiment Trend — {p}\")\n",
    "        plt.xlabel(\"Week\")\n",
    "        plt.ylabel(\"Mean Compound\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        fname = f\"{ARTIFACT_DIR}/trend_{re.sub('[^a-zA-Z0-9]+','_',p)}.png\"\n",
    "        plt.savefig(fname, dpi=140)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpretation Guide (fill in with your observed results)\n",
    "\n",
    "- **Correlation:** A positive Pearson/Spearman correlation indicates that players with more positive draft‑week sentiment tended to perform better on average. If your `r` is close to zero, sentiment may be weakly related to outcomes or confounded by hype/market size.\n",
    "- **precision@k:** Compare the overlap between the top *k* players by sentiment and by performance. A value above baseline (e.g., > 0.20 for large rosters at k=10) suggests sentiment has some predictive power.\n",
    "- **Trends:** Inspect weekly sentiment plots for star players. Spikes may align with news (injuries, depth chart changes, preseason buzz).\n",
    "\n",
    "> **Caveats:** Name‑matching noise, position differences, and small sample sizes can dilute signal. Consider filtering by position or enriching with features like draft slot and team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Suggestions for Real‑World Improvements\n",
    "1. **Better entity linking:** Replace heuristic name matching with a trained NER model and team/position priors; track unique player IDs.\n",
    "2. **De‑duplication & bot filtering:** Remove cross‑posts and obvious bots; use account age/karma thresholds.\n",
    "3. **Richer sentiment:** Combine VADER with transformer sentence‑embedding regressors (finetuned on sports sentiment).\n",
    "4. **Position‑aware performance:** Compute position‑specific performance indices and evaluate per‑position precision@k.\n",
    "5. **Causal checks:** Control for draft position (expected value) to see whether sentiment adds incremental lift."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion (Write‑Up Template)\n",
    "\n",
    "- **What we did:** Preprocessed Reddit data, scored sentiment with VADER, linked comments to players, and compared aggregated sentiment to actual performance.\n",
    "- **What we found:** _(Summarize your correlation/precision values and 2–3 notable players.)_\n",
    "- **So what:** _(State whether sentiment provides actionable signal for scouting/fantasy and any limitations.)_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
